{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple RAG](../../images/simple_rag.png)\n",
    "\n",
    "- LangSmith 문서를 기반으로 응답하는 RAG 구현 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI API key와 LANGCHAIN API key를 프로젝트 루트에 .env 환경변수 파일 생성 후 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Fetching pages: 100%|##########| 219/219 [00:07<00:00, 30.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\" \n",
    "retrieve_documents\n",
    "- 사용자의 질문을 기반으로 벡터스토어에서 가져온 문서를 반환합니다.(* langSmith 문서 기반)\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response  \n",
    "- 입력을 포맷팅한 후 `call_openai`를 호출하여 모델 응답을 생성합니다.\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents): # 질문과 검색된 문서 인자로 받음.\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents) # 검색된 문서 덩어리를 하나의 문서로 변환(포맷팅)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT # 시스템 프롬프트 입력\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\" # 검색된 문서를 Context로 사용자의 질문을 Question으로 전달\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages) # LLM 모델 호출 함수에 메시지 전달(시스템, 유저) 및 응답 결과 반환\n",
    "\n",
    "\"\"\"\n",
    "call_openai  \n",
    "- OpenAI의 채팅 완성 결과를 반환합니다.\n",
    "\"\"\"\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "1. `retrieve_documents`를 호출하여 문서를 가져옵니다.\n",
    "2. 가져온 문서를 기반으로 `generate_response`를 호출하여 응답을 생성합니다.\n",
    "3. 모델 응답을 반환합니다.\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):  # -> 사용자 질문(question)을 인자로 전달\n",
    "    documents = retrieve_documents(question)  # 1. 질문을 기반으로 벡터스토어에서 관련 문서를 검색\n",
    "    response = generate_response(question, documents)  # 2. 질문과 검색된 문서를 기반으로 모델 응답 생성\n",
    "    return response.choices[0].message.content  # 3. 생성된 응답의 콘텐츠 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should take a little less than a minute. We are indexing and storing LangSmith documentation in a SKLearn vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith는 LLM 애플리케이션 개발, 모니터링 및 테스트를 위한 플랫폼입니다. 이 플랫폼은 프로토타입 제작, 디버깅 및 애플리케이션 개발 생애 주기의 각 단계에서 다양한 워크플로우를 지원합니다. 사용자는 LangSmith를 통해 모델 성능을 이해하고 문제를 해결할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"LangSmith는 무엇에 사용되나요?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith 결과 확인\n",
    "\n",
    "<img width=\"1081\" alt=\"image\" src=\"https://github.com/user-attachments/assets/30bf261f-5ec4-4d74-b359-e42f67bc64c9\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
